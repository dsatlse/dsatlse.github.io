+++
title = "Deep Learning"
date = 2020-06-20
[taxonomies]
tags = ["dl"]
+++

# Deep Learning

## Online Courses

+ **Andrew Ng Deep Learning** : https://www.coursera.org/specializations/deep-learning
+ **Fast.Ai** https://www.fast.ai/
+ **UCL x Deepmind Lectures** :  https://www.youtube.com/playlist?list=PLqYmG7hTraZCDxZ44o4p3N5Anz3lLRVZF

## Theory

+ Deep Learning (Goodfellow et al, 2017): https://www.deeplearningbook.org/

+ Grokking Deep Learning (Andrew Trask) : https://www.amazon.co.uk/Deep-Learning-Python-Francois-Chollet/dp/1617294438

## Applications
+ Deep Learning with Python (Francois Chollet) : https://www.amazon.co.uk/Deep-Learning-Python-Francois-Chollet/dp/1617294438

## Image Classification

### InceptionV3
+ [A Simple Guide to the Versions of the Inception Network](https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202)
+ [Review: Inception-v3 — 1st Runner Up (Image Classification) in ILSVRC 2015](#)


## Natural Language Processing
+ [Natural Language Processing is Fun!](https://medium.com/@ageitgey/natural-language-processing-is-fun-9a0bff37854e)
+ [A Practitioner's Guide to Natural Language Processing (Part I) — Processing & Understanding Text](https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72)

Text Model

### RNNs(Recurrent Neural Networks) RNNS & LSTMs (Long Short Term Memory)
+ [Understanding RNN and LSTM](https://towardsdatascience.com/understanding-rnn-and-lstm-f7cdf6dfc14e)
+ [Recurrent Neural Networks and LSTM explained](https://medium.com/@purnasaigudikandula/recurrent-neural-networks-and-lstm-explained-7f51c7f6bbb9)
+ [Recurrent Neural Networks](https://towardsdatascience.com/recurrent-neural-networks-d4642c9bc7ce)
+ [Report on Text Classification using CNN, RNN & HAN](https://medium.com/jatana/report-on-text-classification-using-cnn-rnn-han-f0e887214d5f)
+ [Generating text using a Recurrent Neural Network](https://towardsdatascience.com/generating-text-using-a-recurrent-neural-network-1c3bfee27a5e)
+ [Sentence Prediction Using a Word-level LSTM Text Generator — Language Modeling Using RNN](https://medium.com/towards-artificial-intelligence/sentence-prediction-using-word-level-lstm-text-generator-language-modeling-using-rnn-a80c4cda5b40)
+ [Multi-Class Text Classification with LSTM](https://towardsdatascience.com/multi-class-text-classification-with-lstm-1590bee1bd17)
+ [Illustrated Guide to LSTM’s and GRU’s: A step by step explanation](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)
+ [The magic of LSTM neural networks](https://medium.com/datathings/the-magic-of-lstm-neural-networks-6775e8b540cd)
+ [Video/Course: LongShortTermMemory(LSTM)](https://www.coursera.org/lecture/nlp-sequence-models/long-short-term-memory-lstm-KXoay)

### Term Frequency - Inverse Document Frequency (Tf-Idf)

+ [TF-IDF from scratch in python on real world dataset.](https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089)
+ [What is TF-IDF in Feature Engineering?](https://medium.com/acing-ai/what-is-tf-idf-in-feature-engineering-7f1ba81982bd)
+ [TF IDF | TFIDF Python Example](https://towardsdatascience.com/natural-language-processing-feature-engineering-using-tf-idf-e8b9d00e7e76)
+ [How to process textual data using TF-IDF in Python](https://www.freecodecamp.org/news/how-to-process-textual-data-using-tf-idf-in-python-cd2bbc0a94a3/)
+ [TF-IDF/Term Frequency Technique: Easiest explanation for Text classification in NLP using Python (Chatbot training on words)](https://medium.com/analytics-vidhya/tf-idf-term-frequency-technique-easiest-explanation-for-text-classification-in-nlp-with-code-8ca3912e58c3)

### Word Embeddings
+ [Introduction to Word Embedding and Word2Vec](https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa)
+ [Word embeddings in NLP](https://medium.com/@gunjanagicha/word-embeddings-ee718cd2b8b5)
+ [Video: Using Word Embeddings](https://www.coursera.org/lecture/nlp-sequence-models/using-word-embeddings-qHMK5)


### Sequence-to-Sequence Models

+ [Understanding Encoder-Decoder Sequence to Sequence Model (2019)](https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346)
+ [Sequence To Sequence Models (2018)](https://medium.com/@dhartidhami/sequence-to-sequence-models-3878c0efa557)
+ [Sequence to sequence model: Introduction and concepts (2017)](https://towardsdatascience.com/sequence-to-sequence-model-introduction-and-concepts-44d9b41cd42d)
+ [NLP | Sequence to Sequence Networks| Part 1| Processing text data](https://towardsdatascience.com/nlp-sequence-to-sequence-networks-part-1-processing-text-data-d141a5643b72)
+ [NLP | Sequence to Sequence Networks| Part 2| Seq2seq Model (EncoderDecoder Model)](https://towardsdatascience.com/nlp-sequence-to-sequence-networks-part-2-seq2seq-model-encoderdecoder-model-6c22e29fd7e1)
+ [Sequence Modeling with Deep Learning](https://medium.com/@ODSC/sequence-modelling-with-deep-learning-138dc50c82d2)

### Attention

+ [Brief Introduction to Attention Models](https://towardsdatascience.com/attention-networks-c735befb5e9f)
+ [An introduction to Attention](https://towardsdatascience.com/an-introduction-to-attention-transformers-and-bert-part-1-da0e838c7cda)
+ [Intuitive Understanding of Attention Mechanism in Deep Learning](https://towardsdatascience.com/intuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f)
+ [Attention and its Different Forms](https://towardsdatascience.com/attention-and-its-different-forms-7fc3674d14dc)
+ [Attention Mechanisms in Deep Learning — Not So Special](https://medium.com/retina-ai-health-inc/attention-mechanisms-in-deep-learning-not-so-special-26de2a824f45)
+ [Coursera Video: Attention Model](https://www.coursera.org/lecture/nlp-sequence-models/attention-model-lSwVa)

### Transformers

+ [What is a Transformer?](https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04)
+ [How Transformers Work](https://towardsdatascience.com/transformers-141e32e69591)
+ [Transformer: A Novel Neural Network Architecture for Language Understanding (2017)](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)

### Bidirectional Encoder Representations  from Transformers (BERT)

+ [BERT Explained: State of the art language model for NLP](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)
+ [Understanding BERT: Is it a Game Changer in NLP?](https://towardsdatascience.com/understanding-bert-is-it-a-game-changer-in-nlp-7cca943cf3ad)
+ [Google BERT — Pre Training and Fine Tuning for NLP Tasks](https://medium.com/@ranko.mosic/googles-bert-nlp-5b2bb1236d78)
+ [Building State-of-the-Art Language Models with BERT](https://medium.com/saarthi-ai/bert-how-to-build-state-of-the-art-language-models-59dddfa9ac5d)

## Reinforcement Learning
Reinforcement Learning: An Introduction  (Sutton & Barto, 2018): https://www.amazon.co.uk/Reinforcement-Learning-Introduction-Richard-Sutton/dp/0262039249

## Novel Topics

+ CS 330: Deep Multi-Task and Meta Learning : https://cs330.stanford.edu/

### Cognitive Computing
+ Probabilistic Models of Cognition : https://probmods.org/
+ NYU Computational cognitive modeling - Spring 2020: https://brendenlake.github.io/CCM-site/

